{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install fuzzywuzzy[speedup]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "#import pymorphy2\n",
    "import math\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "import collections\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чтение данных\n",
    "Сет распределен по нескольким файлам, поэтому необходимо их объединить.\n",
    "Сет содержит данные о сложности вопроса по мнению спрашивающего и отвечающего и номер статьи, из которой взят ответ - нам это не нужно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers1 = pd.read_csv('S08_question_answer_pairs.txt', sep='\\t')\n",
    "answers2 = pd.read_csv('S09_question_answer_pairs.txt', sep='\\t')\n",
    "answers3 = pd.read_csv('S10_question_answer_pairs.txt', sep='\\t', encoding='latin-1')\n",
    "\n",
    "answers = pd.concat([answers1, answers2, answers3])\n",
    "answers.drop(labels=['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleFile'], axis=1, inplace=True)\n",
    "answers.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Наличие null:', answers.isnull().values.any())\n",
    "print('Наличие NaN:', answers.isna().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Первичная обработка данных\n",
    "## Обработка дубликатов и пустых значений\n",
    "\n",
    "1. Сет содержит вопросы без ответа\n",
    "2. Сет содержит повторяющиеся вопросы \n",
    "\n",
    "Дубликаты неоходимо удалить - они не несут дополнительной смысловой нагрузки, но увеличивают объем для обработки.\n",
    "Пары с пустыми значениями (без ответа) не могут принеси пользы для модели + нельзя выполнить замену для пустых начени, поскольку необходимо знать правилный ответ. В результате остается только удалить такие вопросы.\n",
    "\n",
    "Сначала удаляем вопросы без ответа, затем повторяющиеся вопросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Размер начального сета', answers.size)\n",
    "\n",
    "#предполагается, что нам необходимо заполнение всех полей\n",
    "not_null_answers = answers.dropna(axis=0, how='any')\n",
    "print('Размер сета без пустых значений', not_null_answers.size)\n",
    "\n",
    "# при удалении дупликатов оставляем только первое вхождение\n",
    "no_duplicates_answers = not_null_answers.drop_duplicates(subset='Question', keep='first')\n",
    "print('Размер сета без дубликатов', no_duplicates_answers.size)\n",
    "no_duplicates_answers.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Наличие null:', no_duplicates_answers.isnull().values.any())\n",
    "print('Наличие NaN:', no_duplicates_answers.isna().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка притяжательных местоимений\n",
    "\n",
    "1. В сете содержатся указания на одушевленный предмет статьи -his/her, he/she \n",
    "#####    (напр. Was his (Alessandro_Volta) 1800 paper written in French ?)\n",
    "2. В сете содержатся указания на неодушевленный предмет статьи -this\n",
    "#####    (напр. \tWhat connected the Akans to this (Ghana) Empire?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Производится замена местоимений на тему, однако только в тех случаях, когда тема не содержится в вопросе и это местоимение является явным на неё указателем: \n",
    "##### (напр. \"When did Lincoln begin his political career?\").\n",
    "В данном случае замена не производится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#no_duplicates_answers['Question'] =no_duplicates_answers['Question'].replace(regex=[r' his | her | this '], \n",
    "#                                                                           value=' замена ')\n",
    "exp = no_duplicates_answers.copy()\n",
    "exp['Theme column'] = ''\n",
    "for index, row in exp.iterrows():\n",
    "    row['ArticleTitle'] = row['ArticleTitle'].replace('_', ' ')\n",
    "        \n",
    "    title_words = list(map(lambda x: x.lower(),row['ArticleTitle'].split()))\n",
    "    \n",
    "    #question_words = re.findall(r\"[a-zA-Z]+-[a-zA-Z]+|[a-zA-Z]+\\'[a-zA-Z]+|[a-zA-Z]+\", row['Question'])\n",
    "    \n",
    "    doc = nlp(row['Question'])\n",
    "    infinitives = [token.lemma_ for token in doc]\n",
    "    '''\n",
    "    infinitives = []\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    for word in question_words:\n",
    "        word = word.lower()\n",
    "        infinitives.append(morph.parse(word)[0].normal_form)        \n",
    "    '''    \n",
    "    \n",
    "    infinitives = list(map(lambda x: x.lower(), infinitives))\n",
    "    has_title_in_question = any(map(lambda x: x in list(map(lambda x: x.lower(), infinitives)), title_words))\n",
    "    \n",
    "    if (not has_title_in_question):      \n",
    "        '''\n",
    "        print(title_words)\n",
    "        print(infinitives)\n",
    "        print(row['ArticleTitle'], row['Question'])\n",
    "        s = row['Question'].lower()        \n",
    "        print(re.sub(r'( he ?)|( his )|( her )|( its )]', ' '+ row['ArticleTitle'] + ' ', ' ' + s + ' ').strip())\n",
    "        ''' \n",
    "        s = row['Question'].lower()  \n",
    "        row['Theme column'] = re.sub(r'( he )|( his )|( her )|( its )]', ' '+ row['ArticleTitle'] + ' ', ' ' + s + ' ').strip()\n",
    "        \n",
    "exp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перенесем все неизмененные значения из столбца Question в Theme column. С последней и будем в дальнейшем работать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in exp.iterrows():\n",
    "    #print(row['Theme column'])\n",
    "    row['Theme column'] = row['Theme column'].lower() if row['Theme column'] != '' else row['Question'].lower() \n",
    "    row['Theme column'] =   row['Theme column'].replace('?','').strip() \n",
    "exp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Перейдем к Токенизации для последующего TF-IDF и WB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = ' '.join(exp['Theme column'])\n",
    "tokens = re.findall(r\"[\\w']+\", text.lower())\n",
    "print('\\nTokens:\\n\\n', tokens[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Счетчик слов:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_dict = Counter(tokens)\n",
    "term_freq_tuple = [(key, value) for key, value in term_freq_dict.items()]\n",
    "sorted(term_freq_tuple, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1, Q2, Q3 = np.percentile(list(term_freq_dict.values()), [25, 50, 75])\n",
    "IQR = Q3 - Q1\n",
    "lower_inner_fence = Q1 - (1.5 * IQR)\n",
    "lower_outer_fence = Q1 - (3 * IQR)\n",
    "upper_inner_fence = Q3 + (1.5 * IQR)\n",
    "upper_outer_fence = Q3 + (3 * IQR)\n",
    "print(f'''upper_outer_fence:  {upper_outer_fence}, \n",
    "upper_inner_fence:  {upper_inner_fence},\n",
    "lower_inner_fence:  {lower_inner_fence},\n",
    "lower_outer_fence\"  {lower_outer_fence}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теперь немного подправим список стоп-слов (но только не значащие). \n",
    "### В нашем случае (в английском языке) — это напр. артикли.\n",
    "### Было решено считать за стоп слова  все слова с частотой при токенизации >50.\n",
    "#### Но с некоторыми исключениями: мы не удаляем уточняющие слова - what/where/e.t.c. (в финальн Более того, можно оставлять в списке только основную форму слова (напр. глагола become), поскольку данные в сете будут Лемматизированны.\n",
    "\n",
    "\n",
    "\n",
    "#### Произведем изменение списка стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = nlp.Defaults.stop_words\n",
    "print('\\nИзначальный набор\\n',len(my_stop_words))\n",
    "#добавление отловленных слов из списка токенизированных, кроме when/where, language \n",
    "my_stop_words|= {'the','of','is','a', 'was','to','are','and','do','for','does','have','as',\n",
    "                 'an','are','many','he','she','it','on','his','her','by','with','that','this','most','at','there','short','long',\n",
    "                '\\'s','.','?','!',',','&'}\n",
    "#удаление лишних слов.  Удаляем только одну форму - become (см выше)\n",
    "my_stop_words-= {'amount', 'another', 'anything', 'become'}\n",
    "#'what',\n",
    "#'when',\n",
    "#'where',\n",
    "#'whereafter',\n",
    "#'which',\n",
    "# 'who',\n",
    "# 'whom',\n",
    "#'whose',\n",
    "#'why',\n",
    " \n",
    "\n",
    "my_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаляем стоп-слова из сета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index, row in exp.iterrows():\n",
    "    new_row = ''\n",
    "    for word in  row['Theme column'].split():\n",
    "        if word not in my_stop_words:\n",
    "            new_row += ' '+word\n",
    "    row['Theme column']=new_row\n",
    "exp.head(50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МЕШОК СЛОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tokens = set(' '.join(tokens).split())\n",
    "vectors = [[sentence.count(token) for token in tokens]\n",
    "           for sentence in tokens]\n",
    "print([(i, word) for i, word in enumerate(full_tokens)])\n",
    "#vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = vectors[1] #choose any\n",
    "for i, sentence in enumerate(tokens):\n",
    "    print(sentence)\n",
    "    print(scipy.spatial.distance.cosine(test_sentence, vectors[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = list('|'.join(text).split('|'))\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "print( vectorizer.fit_transform(corpus).todense() )\n",
    "print( vectorizer.vocabulary_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритм Шинглов — поиск нечетких дубликатов текста\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genshingle(source):\n",
    "    import binascii\n",
    "    shingleLen = 3 #длина шингла - 3--5--7\n",
    "    out = [] \n",
    "    for i in range(len(source)-(shingleLen-1)):\n",
    "        out.append (binascii.crc32(' '.join( [x for x in source[i:i+shingleLen]] ).encode('utf-8')))\n",
    "\n",
    "    return out\n",
    "\n",
    "def compaire(source1,source2):\n",
    "    same = 0\n",
    "    for i in range(len(source1)):\n",
    "        if source1[i] in source2:\n",
    "            same = same + 1\n",
    "\n",
    "    return same*2/float(len(source1) + len(source2))*100\n",
    "\n",
    "def test():\n",
    "    text1 = exp.iloc[0][\"Theme column\"] # Текст 1 для сравнения - abraham lincoln sixteenth president united states\n",
    "    text2 = 'lincoln sixteenth president' # Текст 2 для сравнения - обработанный\n",
    "    text3 = 'was abraham linсoln the 16\\'th president of Ameriсa' # Текст 3 для сравнения -не очищенный\n",
    "    text4 = 'barak obama' # Текст 4 - совсем другой\n",
    "\n",
    "    cmp1 = genshingle(text1)\n",
    "    cmp2 = genshingle(text2)\n",
    "    cmp3 = genshingle(text3)\n",
    "    cmp4 = genshingle(text4)\n",
    "\n",
    "    print ('\\n'+text1)\n",
    "    print (text2)\n",
    "\n",
    "\n",
    "    print ('\\n pretty near '+str(compaire(cmp1,cmp2)))\n",
    "    \n",
    "    print ('\\n'+text1)\n",
    "    print (text3)\n",
    "    print ('\\n Not pretty near '+str(compaire(cmp1,cmp3)))\n",
    "    \n",
    "    print ('\\n'+text1)\n",
    "    print (text4)\n",
    "    print ('\\n Not near almost '+str(compaire(cmp1,cmp4)))\n",
    "\n",
    "# Start program\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF, IDF и TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(text):\n",
    "    #Считаем частотность всех терминов во входном массиве с помощью \n",
    "    #метода Counter библиотеки collections, но уже в очищенной Theme column\n",
    "    tf_text = collections.Counter(text)\n",
    "    for i in tf_text:\n",
    "        tf_text[i] = tf_text[i]/float(len(text))\n",
    "    return tf_text\n",
    "def compute_idf(word, corpus):\n",
    "#на вход берется слово, для которого считаем IDF\n",
    "#и корпус документов в виде списка списков слов\n",
    "        #количество документов, где встречается искомый термин\n",
    "        #считается как генератор списков\n",
    "        return math.log10(len(corpus)/sum([1.0 for i in corpus if word in i]))\n",
    "def compute_idf_another(word, corpus):\n",
    "    data = [Counter(i) for i in corpus if word in i]\n",
    "    final_counter = Counter()\n",
    "    for i in data:\n",
    "        final_counter += i\n",
    "    most_common_word = final_counter.most_common(1)[0][1]\n",
    "    return math.log10(1 + (most_common_word/float(sum([1 for i in corpus if word in i]))))\n",
    "\n",
    "def compute_tfidf(corpus):\n",
    "\n",
    "    documents_list = []\n",
    "    for text in corpus:\n",
    "        tf_idf_dictionary = {}\n",
    "        computed_tf = compute_tf(text)\n",
    "        for word in computed_tf:\n",
    "            tf_idf_dictionary[word] = computed_tf[word] * compute_idf(word, corpus)\n",
    "        documents_list.append(tf_idf_dictionary)\n",
    "    return documents_list\n",
    "\n",
    "def compute_tfidf_another(corpus):\n",
    "\n",
    "    documents_list = []\n",
    "    for text in corpus:\n",
    "        tf_idf_dictionary = {}\n",
    "        computed_tf = compute_tf(text)\n",
    "        for word in computed_tf:\n",
    "            tf_idf_dictionary[word] = computed_tf[word] * compute_idf_another(word, corpus)\n",
    "        documents_list.append(tf_idf_dictionary)\n",
    "    return documents_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = exp[\"Theme column\"]\n",
    "text_separated_by_word = list(map(lambda x: x.split(),'|'.join(text).split('|')))\n",
    "tf=compute_tf(text)\n",
    "#print(tf)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_idf('president',text_separated_by_word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf=compute_tfidf(text_separated_by_word)\n",
    "\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf_another=compute_tfidf_another(text_separated_by_word)\n",
    "\n",
    " \n",
    "print(tfidf_another)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Косинусное расстояние"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#немного переделенные методы для tf-idf \n",
    "import operator\n",
    "\n",
    "\n",
    "def tokenize(doc):\n",
    "    words = [word.replace(',', '').lower() for word in doc.split()]\n",
    "    return words\n",
    "\n",
    "\n",
    "def build_terms(corpus):\n",
    "    terms = {}\n",
    "    current_index = 0\n",
    "    for doc in corpus:\n",
    "        for word in tokenize(doc):\n",
    "            if word not in terms:\n",
    "                terms[word] = current_index\n",
    "                current_index += 1\n",
    "    return terms\n",
    "\n",
    "\n",
    "def tf(document, terms):\n",
    "    words = tokenize(document)\n",
    "    total_words = len(words)\n",
    "    doc_counter = Counter(words)\n",
    "    for word in doc_counter:\n",
    "        # Можно и не делить, а оставить как есть, с частотой\n",
    "        doc_counter[word] /= total_words\n",
    "    tfs = [0 for _ in range(len(terms))]\n",
    "    for term, index in terms.items():\n",
    "        tfs[index] = doc_counter[term]\n",
    "    return tfs\n",
    "\n",
    "\n",
    "def _count_docs_with_word(word, docs):\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        if word in doc:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def idf(documents, terms):\n",
    "    idfs = [0 for _ in range(len(terms))]\n",
    "    total_docs = len(documents)\n",
    "    for word, index in terms.items():\n",
    "        docs_with_word = _count_docs_with_word(word, documents)\n",
    "        idf = 1 + math.log10(total_docs / docs_with_word)\n",
    "        idfs[index] = idf\n",
    "    return idfs\n",
    "\n",
    "\n",
    "def _merge_td_idf(tf, idf, terms):\n",
    "    return [tf[i] * idf[i] for i in range(len(terms))]\n",
    "\n",
    "\n",
    "def build_tfidf(corpus, document, terms):\n",
    "    doc_tf = tf(document, terms)\n",
    "    doc_idf = idf(corpus, terms)\n",
    "    return _merge_td_idf(doc_tf, doc_idf, terms)\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    def dot_product2(v1, v2):\n",
    "        return sum(map(operator.mul, v1, v2))\n",
    "\n",
    "    def vector_cos5(v1, v2):\n",
    "        prod = dot_product2(v1, v2)\n",
    "        len1 = math.sqrt(dot_product2(v1, v1))\n",
    "        len2 = math.sqrt(dot_product2(v2, v2))\n",
    "        return prod / (len1 * len2)\n",
    "         \n",
    "\n",
    "    return vector_cos5(vec1, vec2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_total = []\n",
    "corpus = (tuple(text))[:100]\n",
    "terms = build_terms(corpus)\n",
    "\n",
    "for document in corpus:\n",
    "    tf_idf_total.append(build_tfidf(corpus, document, terms))\n",
    "\n",
    "#for doc_rating in tf_idf_total:\n",
    "    #print(doc_rating)\n",
    "print(terms.keys())\n",
    "query = 'president'\n",
    "print(\"QUERY:\",query )\n",
    "query_tfidf = build_tfidf(corpus, query, terms)\n",
    "for index, document in enumerate(tf_idf_total):\n",
    "    print(\"Similarity with DOC\", index, \"=\", cosine_similarity(query_tfidf, document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Расстояние Ливенштейна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exmpl = exp.iloc[0][\"Theme column\"]\n",
    "print(exmpl)\n",
    "print(fuzz.token_sort_ratio(exmpl, 'lincoln sixteenth president'))\n",
    "print(fuzz.token_set_ratio(exmpl, 'lincoln sixteenth president '))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование моделей для непосредственного поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = 'Did the election of 1880 was won by Lincoln?'\n",
    "\n",
    "#clear input\n",
    "clear_input_exmpl=''\n",
    "new_row = ''\n",
    "\n",
    "#Stop words\n",
    "\n",
    "for word in  input_example.split():\n",
    "   \n",
    "    if word not in my_stop_words:\n",
    "        new_row += ' '+word\n",
    "        clear_input_exmpl=new_row.strip() .lower().replace('?','')\n",
    "\n",
    "print(clear_input_exmpl) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Левенштейн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "levin_counter=0\n",
    "levin_question=''\n",
    "for question in text:\n",
    "    if(fuzz.token_set_ratio(question, clear_input_exmpl)>=levin_counter):\n",
    "        levin_counter=fuzz.token_set_ratio(question, clear_input_exmpl)\n",
    "#       levin_counter=fuzz.token_sort_ratio(question, clear_input_exmpl)\n",
    "        levin_question= question\n",
    "\n",
    "        \n",
    "print('it is most similar to:')\n",
    "print(levin_question)\n",
    "print('Levenshtein distance is:')\n",
    "print(levin_counter)\n",
    "print('Quesion And Answer are:')\n",
    "exp.loc[exp['Theme column'] == levin_question, ['Question','Answer']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Поиск возможных ответов с пониженным расстоянием Левинштейна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "possible_delta= 2\n",
    "all_levenstain_distance = list((map(lambda x: fuzz.token_set_ratio(x, clear_input_exmpl),exp['Theme column'])))\n",
    "                             #  >levin_counter-possible_delta,exp['Theme column']))\n",
    "\n",
    "lev_possible_answers = list(map(lambda x: x>= levin_counter-possible_delta,all_levenstain_distance))\n",
    "lev_possible_answers_with_distance = list(zip(exp['Theme column'][lev_possible_answers],\n",
    "                                          exp['Answer'][lev_possible_answers],\n",
    "                                          pd.Series(all_levenstain_distance)[lev_possible_answers]))\n",
    "print('\\nAlso possible questions are:')\n",
    "lev_possible_answers_with_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шинглы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shingle_counter=0\n",
    "shingle_question=''\n",
    "main_cmp = genshingle(clear_input_exmpl)\n",
    "for question in text:\n",
    "    tmp_cmp = genshingle(question)\n",
    "    similatrity=compaire(main_cmp,tmp_cmp)\n",
    "    if(similatrity>shingle_counter):\n",
    "            shingle_counter=similatrity\n",
    "            shingle_question=question\n",
    "print('it is most similar to:')\n",
    "print(shingle_question)\n",
    "print('Shingle similarity procent is:')\n",
    "print(shingle_counter)\n",
    "print('Quesion And Answer are:')\n",
    "exp.loc[exp['Theme column'] == shingle_question, ['Question','Answer']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Поиск возможных ответов с пониженным показетелем схожести Шинглов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "possible_delta= 0.05\n",
    "all_shingle_distance = list((map(lambda x: compaire(main_cmp,genshingle(x)),exp['Theme column'])))\n",
    "                           \n",
    "\n",
    "Sh_possible_answers = list(map(lambda x: x>= shingle_counter-possible_delta,all_shingle_distance))\n",
    "Sh_possible_answers_with_distance = list(zip(exp['Theme column'][Sh_possible_answers],\n",
    "                                          exp['Answer'][Sh_possible_answers],\n",
    "                                          pd.Series(all_shingle_distance)[Sh_possible_answers]))\n",
    "print('\\nAlso possible questions are:')\n",
    "Sh_possible_answers_with_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('See is it a low coefficient in these words?:')\n",
    "print('\\n for Levenstain')\n",
    "print(pd.Series(tfidf)[lev_possible_answers].values[0])\n",
    "print(pd.Series(tfidf_another)[lev_possible_answers].values[0])\n",
    "###\n",
    "print('\\n for Shingles')\n",
    "print(pd.Series(tfidf)[Sh_possible_answers].values[0])\n",
    "print(pd.Series(tfidf_another)[Sh_possible_answers].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_input_exmpl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка данных на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Очистка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answers_1 = pd.read_csv('test_questions.csv', sep=';', encoding='latin-1')\n",
    "\n",
    "test_answers = pd.concat([test_answers_1])\n",
    "\n",
    "test_exp=test_answers.copy()\n",
    "test_exp['Theme column'] = ''\n",
    "#remove words\n",
    "for index, row in test_exp.iterrows():\n",
    "    row['ArticleTitle'] = row['ArticleTitle'].replace('_', ' ')\n",
    "    row['Answer'] = row['Answer'].lower()    \n",
    "    title_words = list(map(lambda x: x.lower(),row['ArticleTitle'].split()))\n",
    "    \n",
    "    \n",
    "    doc = nlp(row['Question'])\n",
    "    infinitives = [token.lemma_ for token in doc]\n",
    "      \n",
    "    infinitives = list(map(lambda x: x.lower(), infinitives))\n",
    "    has_title_in_question = any(map(lambda x: x in list(map(lambda x: x.lower(), infinitives)), title_words))\n",
    "    \n",
    "    if (not has_title_in_question):      \n",
    "        s = row['Question'].lower()  \n",
    "        row['Theme column'] = re.sub(r'( he )|( his )|( her )|( its )]', ' '+ row['ArticleTitle'] + ' ', ' ' + s + ' ').strip()\n",
    "#move all to Tc\n",
    "for index, row in test_exp.iterrows():\n",
    "    row['Theme column'] = row['Theme column'].lower() if row['Theme column'] != '' else row['Question'].lower() \n",
    "    row['Theme column'] =   row['Theme column'].replace('?','').strip() \n",
    "#Stop words\n",
    "for index, row in test_exp.iterrows():\n",
    "    new_row = ''\n",
    "    for word in  row['Theme column'].split():\n",
    "        if word not in my_stop_words:\n",
    "            new_row += ' '+word\n",
    "    row['Theme column']=new_row    \n",
    "test_exp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Для каждого способа составим вероятный вопрос\n",
    "### В столбцах содержатся вероятные вопросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exp['Fact'] = '' \n",
    "test_exp['Levin_question'] = '' #предикт вопроса по левинштейну\n",
    "test_exp['Levin_answer'] = ''\n",
    "test_exp['Shingles_question'] = '' #предикт вопроса по шинглам\n",
    "test_exp['Shingles_answer'] = ''\n",
    "\n",
    "for index, row in test_exp.iterrows():\n",
    "    \n",
    "    #levin\n",
    "    levin_counter=0\n",
    "    levin_question=''\n",
    "    for question in text:\n",
    "        if(fuzz.token_set_ratio(question, row['Theme column'])>=levin_counter):\n",
    "            levin_counter=fuzz.token_set_ratio(question, row['Theme column'])\n",
    "            levin_question= question\n",
    "    row['Levin_question'] = str(exp.loc[exp['Theme column'] == levin_question, ['Question']].values[0]).replace('[\\'','').replace('\\']','').replace('[\\\"','').replace('\\\"]','').lower()\n",
    "    row['Levin_answer'] = str(exp.loc[exp['Theme column'] == levin_question, ['Answer']].values[0]).replace('[\\'','').replace('\\']','').replace('[\\\"','').replace('\\\"]','').lower()\n",
    "    \n",
    "    # shingle\n",
    "    shingle_counter=0\n",
    "    shingle_question=''\n",
    "    main_cmp = genshingle(row['Theme column'])\n",
    "    for question in text:\n",
    "        tmp_cmp = genshingle(question)\n",
    "        similatrity=compaire(main_cmp,tmp_cmp)\n",
    "        if(similatrity>shingle_counter):\n",
    "            shingle_counter=similatrity\n",
    "            shingle_question=question\n",
    "    #print(shingle_question)      \n",
    "    row['Shingles_question'] = str(exp.loc[exp['Theme column'] == shingle_question, ['Question']].values[0]).replace('[\\'','').replace('\\']','').lower()\n",
    "    row['Shingles_answer'] = str(exp.loc[exp['Theme column'] == shingle_question, ['Answer']].values[0]).replace('[\\'','').replace('\\']','').lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поскольку cos-ое расстояние и список всех векторов требует неадекватно много времени:\n",
    "#### Сравним алгоритмы со стандартным-реализованным решением\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def similarity(s1, s2):\n",
    "    normalized1 = s1.lower()\n",
    "    normalized2 = s2.lower()\n",
    "    matcher = difflib.SequenceMatcher(None, normalized1, normalized2)\n",
    "    return matcher.ratio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exp['difflib_question'] = '' #предикт вопроса по difflib\n",
    "test_exp['difflib_answer'] = ''\n",
    "difflib_counter=0\n",
    "difflib_question=''\n",
    "for index, row in test_exp.iterrows():\n",
    "    for question in text:\n",
    "        if(similarity(question, row['Theme column'])>=difflib_counter):\n",
    "            difflib_counter=similarity(question, row['Theme column'])\n",
    "            difflib_question= question\n",
    "    row['difflib_question'] = str(exp.loc[exp['Theme column'] == difflib_question, ['Question']].values[0]).replace('[\\'','').replace('\\']','').replace('[\\\"','').replace('\\\"]','').lower()\n",
    "    row['difflib_answer'] = str(exp.loc[exp['Theme column'] == difflib_question, ['Answer']].values[0]).replace('[\\'','').replace('\\']','').replace('[\\\"','').replace('\\\"]','').lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exp.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 если ответы не сошлись, 1 если сошлись\n",
    "test_exp['Levin_count'] = ''\n",
    "test_exp['Shingle_count'] = ''\n",
    "test_exp['difflib_count'] = ''\n",
    "for index, row in test_exp.iterrows():\n",
    "    \n",
    "    if(row['Levin_answer']==row['Answer']):\n",
    "        row['Levin_count']=1\n",
    "    else:\n",
    "        row['Levin_count']=0\n",
    "\n",
    "    if(row['Shingles_answer']==row['Answer']):\n",
    "        row['Shingle_count']=1\n",
    "    else:\n",
    "        row['Shingle_count']=0\n",
    "    \n",
    "    if(row['difflib_answer']==row['Answer']):\n",
    "        row['difflib_count']=1\n",
    "    else:\n",
    "        row['difflib_count']=0\n",
    "    row['Fact']=str(exp.loc[exp['Question'] == 'Did Lincoln start his political career in 1832?', ['Answer']]).split()[2].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exp.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Левинштейн точность')\n",
    "test_exp['Levin_count'].sum()/test_exp['Levin_count'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Шинглы точность')\n",
    "test_exp['Shingle_count'].sum()/test_exp['Shingle_count'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('difflib точность')\n",
    "test_exp['difflib_count'].sum()/test_exp['difflib_count'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exp.to_csv('result.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
